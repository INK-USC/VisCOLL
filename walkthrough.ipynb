{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VisCOLL: training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configs can be specified in files such as files under configs/mlmcaptioning/. or directly specified in the command line. Some configurations that should be specified are\n",
    "- Model achitecture: cfg.MLMCAPTION.BASE=vlbert/lxmert\n",
    "- Output dir (cfg.OUTPUT_DIR)\n",
    "- Replay memory size (cfg.EXTERNAL.REPLAY.MEM_LIMIT)\n",
    "- OCL algorithm (cfg.EXTERNAL.OCL.ALGO=naive|ER|AGEM)\n",
    "    - To run MIR, specify ALGO=ER and let EXTERNAL.OCL.MIR=1. Also, you should specify the hyperparameter EXTERNAL.OCL.MIR_K. Finally, EXTERNAL.OCL.MIR_AGG decides whether use original MIR or the variant MIR-MAX.\n",
    "\n",
    "For example, to train a VLBERT model with a memory of 10,000 examples on coco using ER continual learning algorithm, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "/home/xsjin/viscoll_release/nets/VLBERT/pretrain/function/config.py:181: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  exp_config = edict(yaml.load(f))\n",
      "epoch\n",
      "loading from cache\n",
      "** Buffer details **\n",
      "* length: 638903\n",
      "* task: continuous\n",
      "loading from cache\n",
      "** Buffer details **\n",
      "* length: 28720\n",
      "* task: continuous\n",
      "  0%|                                                 | 0/19966 [00:00<?, ?it/s]continuous 19966\n",
      "  5%|█▊                                   | 989/19966 [09:41<4:47:33,  1.10it/s]"
     ]
    }
   ],
   "source": [
    "!python train_mlm.py --name debug --config configs/mlmcaptioning/er.yaml --seed 0 --cfg MLMCAPTION.BASE=vlbert OUTPUT_DIR=runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a LXMERT model with a memory of 10,000 examples on Flickr using AGEM, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python train_mlm.py --name debug_flickr --config configs/mlmcaptioning/agem_flickr.yaml --seed 0 --cfg MLMCAPTION.BASE=lxmert OUTPUT_DIR=runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a VLBERT model with a memory of 10,000 examples on COCO, using MIR and MIR_K=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python train_mlm.py --name debug_mir --config configs/mlmcaptioning/er_mir.yaml --seed 0 --cfg MLMCAPTION.BASE=vlbert OUTPUT_DIR=runs/\n",
    "#equivalent to !python train_mlm.py --name debug_mir --config configs/mlmcaptioning/er.yaml --seed 0 --cfg MLMCAPTION.BASE=vlbert OUTPUT_DIR=runs/ EXTERNAL.OCL.MIR=1 EXTERNAL.OCL.MIR_AGG=max EXTERNAL.OCL.MIR_K=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will ends training and saves itself to \"<output_dir\\>/model00.pth\" after the first pass of the data (the online continual learning setup). It will also outputs model checkpoints every 2000 iterations in \"<output_dir>/results/model_0_<iter\\>.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate a model, you may run test.py. In the command line, You can specify epoch and iter. For COCO, you can also specify the \"--novel_comps\" flag to evaluate on 24 heldout concept pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --name debug --config configs/mlmcaptioning/er.yaml --seed 0 --epoch 00 --cfg MLMCAPTION.BASE=vlbert OUTPUT_DIR=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --name debug --config configs/mlmcaptioning/er.yaml --seed 0 --epoch 00 --novel_comps --cfg MLMCAPTION.BASE=vlbert OUTPUT_DIR=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can run evaluation on Flickr-shift. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --name flickr-lxmert-er-mem10k-lr0.0001 --config configs/mlmcaptioning/er_flickr.yaml --seed 1 --epoch 00 --cfg MLMCAPTION.BASE=lxmert OUTPUT_DIR=runs-bak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute metrics such as forgetting, running evaluation for every intermediate checkpoint file is required. You can use the shell script in scripts/ folder to perform batch processing for all seeds and all checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/mlm_eval.sh\n",
    "# or !python ./scripts/mlm_eval_flickr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluting continual learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running inferences, you will find a file named \"results_verbose_model_<epoch\\>_<iter\\>.json\" in the output directory. The file contains raw predictions and scores to compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final BLEU and PPL score\n",
    "from metrics.final_scores import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final BLEU 1 to BLEU 4\n",
    "evaluate_bleu_score_from_records('runs-bak/flickr-vlbert-er-mem10k-lr0.0001_1/results_verbose_model_00.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final log PPL\n",
    "evaluate_ppl_from_records('runs-bak/flickr-vlbert-er-mem10k-lr0.0001_1/results_verbose_model_00.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also run the forgetting metrics reported in the paper. This requires knowing when the task is visited in the training data stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.coco import COCO\n",
    "from yacs.config import CfgNode\n",
    "\n",
    "cfg = CfgNode(new_allowed=True)\n",
    "cfg.merge_from_file('configs/mlmcaptioning/naive.yaml') # placeholder config file\n",
    "coco_dataset = COCO(cfg=cfg, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_tasks = coco_dataset.all_tasks\n",
    "coco_dataset.set_task('continuous')\n",
    "coco_buffer = coco_dataset.current_task_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.forgetting import *\n",
    "task_ends = stat_task_end(coco_tasks, coco_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_forgetting_single(task_ends, coco_tasks, output_dir='runs-bak/mscoco-vlbert-er-mem10k-lr0.0001_1', dataset='coco')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating compositional generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import metrics.comp_gen\n",
    "importlib.reload(metrics.comp_gen)\n",
    "from metrics.comp_gen import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First tokenize sentences in the coco dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_buffer = tokenize_buffer(coco_buffer, coco_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24 novel pairs are defined in metrics/comp_gen.py. We construct corresponding \"seen pairs\" with the same set of atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_pairs = get_possible_seen_pairs_from_novel_pairs(novel_pairs, coco_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes performance on novel pairs on the compositional test split and seens pairs on the regular test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, seen_pairs_ppl, _, _, _, _ = compute_novel_seen_performance_verbose('runs-bak/mscoco-vlbert-er-mem10k-lr0.0001_1', 'results_verbose_model_00.json', novel_pairs, possible_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_pairs_ppl, _, _, _, _, _ = compute_novel_seen_performance_verbose('runs-bak/mscoco-vlbert-er-mem10k-lr0.0001_1', 'results_verbose_model_00_novel_comps.json', novel_pairs, possible_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_pairs_ppl, novel_pairs_ppl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
